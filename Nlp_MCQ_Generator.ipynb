{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxudmUDrBNRQ",
        "outputId": "d182143f-9aea-4e95-993f-9b3a8ce06cf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `pip install git+https://github.com/boudinfl/pke.gitfile=open(\"ahistoryofdepression.txt\",\"r\") #\"r\" deontes read version open'\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/boudinfl/pke.gitfile=open(\"ahistoryofdepression.txt\",\"r\") #\"r\" deontes read version open"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/boudinfl/pke.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB5_DgvMWf4y",
        "outputId": "c07d953e-7217-4e03-9a5e-f016b8bbbb16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/boudinfl/pke.git\n",
            "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-3rk3hc0k\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/boudinfl/pke.git /tmp/pip-req-build-3rk3hc0k\n",
            "  Resolved https://github.com/boudinfl/pke.git to commit 69871ffdb720b83df23684fea53ec8776fd87e63\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.8.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.2.2)\n",
            "Collecting unidecode (from pke==2.0.0)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (0.18.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.4.0)\n",
            "Requirement already satisfied: spacy>=3.2.3 in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pke==2.0.0) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pke==2.0.0) (2023.12.25)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pke==2.0.0) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.2.3->pke==2.0.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.3->pke==2.0.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.3->pke==2.0.0) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.2.3->pke==2.0.0) (1.1.0)\n",
            "Building wheels for collected packages: pke\n",
            "  Building wheel for pke (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pke: filename=pke-2.0.0-py3-none-any.whl size=6160628 sha256=1c0f91f0b49f65d6c0254f552c2e41d2221db3db20ffd521089c5d9f7169f304\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jpssyzb4/wheels/8c/07/29/6b35bed2aa36e33d77ff3677eb716965ece4d2e56639ad0aab\n",
            "Successfully built pke\n",
            "Installing collected packages: unidecode, pke\n",
            "Successfully installed pke-2.0.0 unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pke"
      ],
      "metadata": {
        "id": "DE7ZkXsmCboM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file=open(\"/content/five_rules.txt\",\"r\") #\"r\" deontes read version open\n",
        "text=file.read()"
      ],
      "metadata": {
        "id": "ZhwIVZCeC1S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('popular')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWMirMuYD31I",
        "outputId": "72fcb256-cbc4-426a-e995-42e9884f0984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade -q gin git+https://github.com/google/trax.git@v1.2.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2JqzMGkEqmP",
        "outputId": "6ae62cf7-b047-42e2-9392-2019d3441917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.4/193.4 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.2/385.2 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.1/367.1 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.4/981.4 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.5/230.5 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.8/174.8 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.0/165.0 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.2/151.2 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.8/247.8 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.5/191.5 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.4/191.4 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.2/178.2 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for trax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords #Stopwords are the words that we need to avoid while considering keyword extraction\n",
        "import string\n",
        "def getImportantWords(art):\n",
        "    extractor=pke.unsupervised.MultipartiteRank() #Using the Multipartite Unsupervised Extractor as our extractor\n",
        "    extractor.load_document(input=art,language='en')\n",
        "    pos={'PROPN'} #We are only considering proper nouns as valid candidates for our keywords\n",
        "    stops=list(string.punctuation) #Stoplist contains the words to be avoided\n",
        "    stops+=['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-'] #These stand for the brackets as in lrb=left round bracket=\"(\" and so on\n",
        "    stops+=stopwords.words('english')\n",
        "    extractor.candidate_selection() #Sets the candidate selection criteria, as in, which should be considered and which should be avoided\n",
        "    extractor.candidate_weighting() #Sets the preference criteria for the candidates\n",
        "    result=[]\n",
        "    ex=extractor.get_n_best(n=25) #Gets the 25 best candidates according to the criteria set\n",
        "    for each in ex:\n",
        "        result.append(each[0])\n",
        "    return result\n",
        "impWords=getImportantWords(text) #Get the important words (keywords) from the text article using the above function\n",
        "#print(impWords)\n",
        "\n"
      ],
      "metadata": {
        "id": "3nbxk2DrD4zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(impWords)\n"
      ],
      "metadata": {
        "id": "EhNsRxSFD413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a01b5b82-4692-41d3-8b88-d050553e2808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['massive data sets', 'humans', 'ai techniques', 'alphabet', 'enterprises insights', 'number', 'relevant fields', 'company', 'operations', 'tasks', 'customer service', 'business', 'data', 'important', 'central', 'example', 'lead generation', 'fraud detection', 'large amounts', 'areas', 'legal documents', 'ability', 'apple', 'quality control', 'marketing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "def splitTextToSents(art):\n",
        "    s=[sent_tokenize(art)]\n",
        "    s=[y for x in s for y in x]\n",
        "    s=[sent.strip() for sent in s if len(sent)>15] #Removes all the sentences that have length less than 15 so that we can ensure that our questions have enough length for context\n",
        "    return s\n",
        "sents=splitTextToSents(text) #Achieve a well splitted set of sentences from the text article\n",
        "#print(sents)"
      ],
      "metadata": {
        "id": "NiMAWDcfD44D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sents)"
      ],
      "metadata": {
        "id": "2_RxUWaFD45s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9521ab5a-322e-42eb-ec6f-b1a0a8210e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AI is important for its potential to change how we live, work and play.', 'It has been effectively used in business to automate tasks traditionally done by humans, including customer service, lead generation, fraud detection and quality control.', 'In a number of areas, AI can perform tasks more efficiently and accurately than humans.', 'It is especially useful for repetitive, detail-oriented tasks such as analyzing large numbers of legal documents to ensure relevant fields are properly filled in.', \"AI's ability to process massive data sets gives enterprises insights into their operations they might not otherwise have noticed.\", 'The rapidly expanding array of generative AI tools is also becoming important in fields ranging from education to marketing to product design.', 'Advances in AI techniques have not only helped fuel an explosion in efficiency, but also opened the door to entirely new business opportunities for some larger enterprises.', 'Prior to the current wave of AI, for example, it would have been hard to imagine using computer software to connect riders to taxis on demand, yet Uber has become a Fortune 500 company by doing just that.', \"AI has become central to many of today's largest and most successful companies, including Alphabet, Apple, Microsoft and Meta, which use AI to improve their operations and outpace competitors.\", 'At Alphabet subsidiary Google, for example, AI is central to its eponymous search engine, and self-driving car company Waymo began as an Alphabet division.', \"The Google Brain research lab also invented the transformer architecture that underpins recent NLP breakthroughs such as OpenAI's ChatGPT.\", 'AI technologies, particularly deep learning models such as artificial neural networks, can process large amounts of data much faster and make predictions more accurately than humans can.', 'While the huge volume of data created on a daily basis would bury a human researcher, AI applications using machine learning can take that data and quickly turn it into actionable information.', 'A primary disadvantage of AI is that it is expensive to process the large amounts of data AI requires.', \"As AI techniques are incorporated into more products and services, organizations must also be attuned to AI's potential to create biased and discriminatory systems, intentionally or inadvertently.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flashtext"
      ],
      "metadata": {
        "id": "yeu6hcTqFk39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "603fcdf9-bc90-4fe5-e06e-5bf58a2349db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flashtext\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9296 sha256=c78dead8a8c764c688bda8a3e969265e218a77ecbf59eae2cde8cb93997cc197\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/be/39/c37ad168eb2ff644c9685f52554440372129450f0b8ed203dd\n",
            "Successfully built flashtext\n",
            "Installing collected packages: flashtext\n",
            "Successfully installed flashtext-2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flashtext import KeywordProcessor\n",
        "def mapSents(impWords,sents):\n",
        "    processor=KeywordProcessor() #Using keyword processor as our processor for this task\n",
        "    keySents={}\n",
        "    for word in impWords:\n",
        "        keySents[word]=[]\n",
        "        processor.add_keyword(word) #Adds key word to the processor\n",
        "    for sent in sents:\n",
        "        found=processor.extract_keywords(sent) #Extract the keywords in the sentence\n",
        "        for each in found:\n",
        "            keySents[each].append(sent) #For each keyword found, map the sentence to the keyword\n",
        "    for key in keySents.keys():\n",
        "        temp=keySents[key]\n",
        "        temp=sorted(temp,key=len,reverse=True) #Sort the sentences according to their decreasing length in order to ensure the quality of question for the MCQ\n",
        "        keySents[key]=temp\n",
        "    return keySents\n",
        "mappedSents=mapSents(impWords,sents) #Achieve the sentences that contain the keywords and map those sentences to the keywords using this function\n",
        "#print(mappedSents)"
      ],
      "metadata": {
        "id": "nWU_k5v7FeUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mappedSents)"
      ],
      "metadata": {
        "id": "b66_a4frFwQ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5171340c-fbeb-4456-c5d3-1415316191de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'massive data sets': [\"AI's ability to process massive data sets gives enterprises insights into their operations they might not otherwise have noticed.\"], 'humans': ['AI technologies, particularly deep learning models such as artificial neural networks, can process large amounts of data much faster and make predictions more accurately than humans can.', 'It has been effectively used in business to automate tasks traditionally done by humans, including customer service, lead generation, fraud detection and quality control.', 'In a number of areas, AI can perform tasks more efficiently and accurately than humans.'], 'ai techniques': [\"As AI techniques are incorporated into more products and services, organizations must also be attuned to AI's potential to create biased and discriminatory systems, intentionally or inadvertently.\", 'Advances in AI techniques have not only helped fuel an explosion in efficiency, but also opened the door to entirely new business opportunities for some larger enterprises.'], 'alphabet': [\"AI has become central to many of today's largest and most successful companies, including Alphabet, Apple, Microsoft and Meta, which use AI to improve their operations and outpace competitors.\", 'At Alphabet subsidiary Google, for example, AI is central to its eponymous search engine, and self-driving car company Waymo began as an Alphabet division.', 'At Alphabet subsidiary Google, for example, AI is central to its eponymous search engine, and self-driving car company Waymo began as an Alphabet division.'], 'enterprises insights': [\"AI's ability to process massive data sets gives enterprises insights into their operations they might not otherwise have noticed.\"], 'number': ['In a number of areas, AI can perform tasks more efficiently and accurately than humans.'], 'relevant fields': ['It is especially useful for repetitive, detail-oriented tasks such as analyzing large numbers of legal documents to ensure relevant fields are properly filled in.'], 'company': ['Prior to the current wave of AI, for example, it would have been hard to imagine using computer software to connect riders to taxis on demand, yet Uber has become a Fortune 500 company by doing just that.', 'At Alphabet subsidiary Google, for example, AI is central to its eponymous search engine, and self-driving car company Waymo began as an Alphabet division.'], 'operations': [\"AI has become central to many of today's largest and most successful companies, including Alphabet, Apple, Microsoft and Meta, which use AI to improve their operations and outpace competitors.\", \"AI's ability to process massive data sets gives enterprises insights into their operations they might not otherwise have noticed.\"], 'tasks': ['It has been effectively used in business to automate tasks traditionally done by humans, including customer service, lead generation, fraud detection and quality control.', 'It is especially useful for repetitive, detail-oriented tasks such as analyzing large numbers of legal documents to ensure relevant fields are properly filled in.', 'In a number of areas, AI can perform tasks more efficiently and accurately than humans.'], 'customer service': ['It has been effectively used in business to automate tasks traditionally done by humans, including customer service, lead generation, fraud detection and quality control.'], 'business': ['Advances in AI techniques have not only helped fuel an explosion in efficiency, but also opened the door to entirely new business opportunities for some larger enterprises.', 'It has been effectively used in business to automate tasks traditionally done by humans, including customer service, lead generation, fraud detection and quality control.'], 'data': ['While the huge volume of data created on a daily basis would bury a human researcher, AI applications using machine learning can take that data and quickly turn it into actionable information.', 'While the huge volume of data created on a daily basis would bury a human researcher, AI applications using machine learning can take that data and quickly turn it into actionable information.', 'AI technologies, particularly deep learning models such as artificial neural networks, can process large amounts of data much faster and make predictions more accurately than humans can.', 'A primary disadvantage of AI is that it is expensive to process the large amounts of data AI requires.'], 'important': ['The rapidly expanding array of generative AI tools is also becoming important in fields ranging from education to marketing to product design.', 'AI is important for its potential to change how we live, work and play.'], 'central': [\"AI has become central to many of today's largest and most successful companies, including Alphabet, Apple, Microsoft and Meta, which use AI to improve their operations and outpace competitors.\", 'At Alphabet subsidiary Google, for example, AI is central to its eponymous search engine, and self-driving car company Waymo began as an Alphabet division.'], 'example': ['Prior to the current wave of AI, for example, it would have been hard to imagine using computer software to connect riders to taxis on demand, yet Uber has become a Fortune 500 company by doing just that.', 'At Alphabet subsidiary Google, for example, AI is central to its eponymous search engine, and self-driving car company Waymo began as an Alphabet division.'], 'lead generation': ['It has been effectively used in business to automate tasks traditionally done by humans, including customer service, lead generation, fraud detection and quality control.'], 'fraud detection': ['It has been effectively used in business to automate tasks traditionally done by humans, including customer service, lead generation, fraud detection and quality control.'], 'large amounts': ['AI technologies, particularly deep learning models such as artificial neural networks, can process large amounts of data much faster and make predictions more accurately than humans can.', 'A primary disadvantage of AI is that it is expensive to process the large amounts of data AI requires.'], 'areas': ['In a number of areas, AI can perform tasks more efficiently and accurately than humans.'], 'legal documents': ['It is especially useful for repetitive, detail-oriented tasks such as analyzing large numbers of legal documents to ensure relevant fields are properly filled in.'], 'ability': [\"AI's ability to process massive data sets gives enterprises insights into their operations they might not otherwise have noticed.\"], 'apple': [\"AI has become central to many of today's largest and most successful companies, including Alphabet, Apple, Microsoft and Meta, which use AI to improve their operations and outpace competitors.\"], 'quality control': ['It has been effectively used in business to automate tasks traditionally done by humans, including customer service, lead generation, fraud detection and quality control.'], 'marketing': ['The rapidly expanding array of generative AI tools is also becoming important in fields ranging from education to marketing to product design.']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pywsd"
      ],
      "metadata": {
        "id": "_WvC2I4JGGBz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b9b165f-53b1-4f51-811a-acac2a310a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pywsd\n",
            "  Downloading pywsd-1.2.5-py3-none-any.whl (26.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pywsd) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pywsd) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pywsd) (2.0.3)\n",
            "Collecting wn==0.0.23 (from pywsd)\n",
            "  Downloading wn-0.0.23.tar.gz (31.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.6/31.6 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pywsd) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pywsd) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->pywsd) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pywsd) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->pywsd) (4.66.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->pywsd) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pywsd) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pywsd) (2024.1)\n",
            "Building wheels for collected packages: wn\n",
            "  Building wheel for wn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wn: filename=wn-0.0.23-py3-none-any.whl size=31792911 sha256=142f08e6885c6593415f9e028bf84bd88bea9505d8899ddb997ee02d667af750\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/1a/7d/23a76ce45998af60e47466a694c237fa26023c5674b47672b2\n",
            "Successfully built wn\n",
            "Installing collected packages: wn, pywsd\n",
            "Successfully installed pywsd-1.2.5 wn-0.0.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pywsd.similarity import max_similarity\n",
        "from pywsd.lesk import adapted_lesk\n",
        "from nltk.corpus import wordnet as wn\n",
        "def getWordSense(sent,word):\n",
        "    word=word.lower()\n",
        "    if len(word.split())>0: #Splits the word with underscores(_) instead of spaces if there are multiple words\n",
        "        word=word.replace(\" \",\"_\")\n",
        "    synsets=wn.synsets(word,'n') #Sysnets from Google are invoked\n",
        "    if synsets:\n",
        "        wup=max_similarity(sent,word,'wup',pos='n')\n",
        "        adapted_lesk_output = adapted_lesk(sent, word, pos='n')\n",
        "        lowest_index=min(synsets.index(wup),synsets.index(adapted_lesk_output))\n",
        "        return synsets[lowest_index]\n",
        "    else:\n",
        "        return None\n",
        "#print(\"fin\")"
      ],
      "metadata": {
        "id": "eLV30hn7FzYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed0c178c-39a3-4c18-dedc-8e44a35ce14e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warming up PyWSD (takes ~10 secs)... took 4.8746254444122314 secs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getDistractors(syn,word):\n",
        "    dists=[]\n",
        "    word=word.lower()\n",
        "    actword=word\n",
        "    if len(word.split())>0: #Splits the word with underscores(_) instead of spaces if there are multiple words\n",
        "        word.replace(\" \",\"_\")\n",
        "\n",
        "    hypernym = syn.hypernyms() #Gets the hypernyms of the word\n",
        "    if len(hypernym)==0: #If there are no hypernyms for the current word, we simple return the empty list of distractors\n",
        "        return dists\n",
        "    for each in hypernym[0].hyponyms(): #Other wise we find the relevant hyponyms for the hypernyms\n",
        "        name=each.lemmas()[0].name()\n",
        "        if(name==actword):\n",
        "            continue\n",
        "        name=name.replace(\"_\",\" \")\n",
        "        name=\" \".join(w.capitalize() for w in name.split())\n",
        "        if name is not None and name not in dists: #If the word is not already present in the list and is different from he actial word\n",
        "            dists.append(name)\n",
        "    return dists"
      ],
      "metadata": {
        "id": "gPR2dUkuGb1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "def getDistractors2(word):\n",
        "    word=word.lower()\n",
        "    actword=word\n",
        "    if len(word.split())>0: #Splits the word with underscores(_) instead of spaces if there are multiple words\n",
        "        word=word.replace(\" \",\"_\")\n",
        "    dists=[]\n",
        "    url= \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,word) #To get ditractors from ConceptNet's API\n",
        "    obj=requests.get(url).json()\n",
        "    for edge in obj['edges']:\n",
        "        link=edge['end']['term']\n",
        "        url2=\"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n",
        "        obj2=requests.get(url2).json()\n",
        "        for edge in obj2['edges']:\n",
        "            word2=edge['start']['label']\n",
        "            if word2 not in dists and actword.lower() not in word2.lower(): #If the word is not already present in the list and is different from he actial word\n",
        "                dists.append(word2)\n",
        "    return dists"
      ],
      "metadata": {
        "id": "GXozWTJjGiws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mappedDists={}\n",
        "for each in mappedSents:\n",
        "    wordsense=getWordSense(mappedSents[each][0],each) #gets the sense of the word\n",
        "    if wordsense: #if the wordsense is not null/none\n",
        "        dists=getDistractors(wordsense,each) #Gets the WordNet distractors\n",
        "        if len(dists)==0: #If there are no WordNet distractors available for the current word\n",
        "            dists=getDistractors2(each) #The gets the distractors from the ConceptNet API\n",
        "        if len(dists)!=0: #If there are indeed distractors from WordNet available, then maps them\n",
        "            mappedDists[each]=dists\n",
        "    else: #If there is no wordsense, the directly searches/uses the ConceptNet\n",
        "        dists=getDistractors2(each)\n",
        "        if len(dists)>0: #If it gets the Distractors then maps them\n",
        "            mappedDists[each]=dists\n",
        "#print(mappedDists)"
      ],
      "metadata": {
        "id": "GtrUp_5aGo3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mappedDists)"
      ],
      "metadata": {
        "id": "TZg46z9mGuvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91346927-a5a4-4aa7-dfba-55889d75879f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'humans': ['Australopithecine', 'Dryopithecine', 'Homo', 'Javanthropus', 'Pithecanthropus', 'Sinanthropus', 'Sivapithecus'], 'alphabet': ['Alphanumerics'], 'number': ['Alarm', 'All Clear', 'Animal Communication', 'Bugle Call', 'Curfew', 'Distress Signal', 'Dog-ear', 'Drumbeat', 'Electronic Signal', 'High Sign', 'Indicator', 'Input Signal', 'Output Signal', 'Phone Number', 'Radio Beacon', 'Radio Beam', 'Recording', 'Retreat', 'Starting Signal', 'Storm Signal', 'Symbol', 'Telegraphic Signal', 'Ticktack', 'Time Signal', 'Visual Signal', 'Whistle'], 'company': ['Boulevardier', 'Caller', 'Guest', 'Visiting Fireman'], 'operations': ['Acting', 'Aid', 'Attempt', 'Behavior', 'Burst', 'Buzz', 'Calibration', 'Ceremony', 'Concealment', 'Continuance', 'Control', 'Creation', 'Cup Of Tea', 'Demand', 'Dismantling', 'Diversion', 'Domesticity', 'Education', 'Energizing', 'Enjoyment', 'Follow-up', 'Fun', 'Game', 'Grouping', 'Lamentation', 'Last', 'Laughter', 'Leadership', 'Liveliness', 'Market', 'Measurement', 'Music', 'Mystification', 'Negotiation', 'Occupation', 'Operation', 'Organization', 'Perturbation', 'Placement', 'Pleasure', 'Politics', 'Practice', 'Precession', 'Preparation', 'Procedure', 'Protection', 'Provision', 'Puncture', 'Release', 'Representation', 'Role', 'Search', 'Sensory Activity', 'Service', 'Sin', 'Solo', 'Space Walk', 'Support', 'Timekeeping', 'Training', 'Turn', 'Use', 'Variation', 'Verbalization', 'Waste', 'Work', 'Works', 'Worship', 'Writing', 'Wrongdoing'], 'tasks': ['Action', 'Busywork', 'Care', 'Coursework', 'Duty', 'Heavy Lifting', 'Housewifery', 'Housework', 'Investigation', 'Ironing', 'Job', 'Labor', 'Logging', 'Loose End', 'Mission', 'Nightwork', 'Operation', 'Paperwork', 'Service', 'Shining', 'Spadework', 'Subbing', 'Timework', 'Undertaking', 'Wash', 'Welfare Work'], 'business': ['Collective', 'Commercial Enterprise', 'Giant'], 'data': ['Agglomeration', 'Ana', 'Armamentarium', 'Art Collection', 'Asia', 'Assortment', 'Aviation', 'Backlog', 'Batch', 'Battery', 'Biota', 'Block', 'Book', 'Bottle Collection', 'Bunch', 'Central America', 'Class', 'Coin Collection', 'Collage', 'Combination', 'Congregation', 'Content', 'Convoy', 'Corpus', 'Crop', 'Defense', 'Ensemble', 'Europe', 'Exhibition', 'Fauna', 'Findings', 'Flagging', 'Fleet', 'Flinders', 'Free World', 'Galaxy', 'Generally Accepted Accounting Principles', 'Gimmickry', 'Hand', 'Herbarium', 'Hit Parade', 'Job Lot', 'Judaica', 'Kludge', 'Law', 'Library', 'Long Suit', 'Mail', 'Mass', 'Menagerie', 'Mythology', 'North America', 'Nuclear Club', 'Oort Cloud', 'Pack', 'Package', 'Pantheon', 'Petting Zoo', 'Pharmacopoeia', 'Pile', 'Planting', 'Population', 'Procession', 'Prosecution', 'Repertoire', 'Repertory', \"Rogue's Gallery\", 'Set', 'Signage', 'Smithereens', 'South America', 'Stamp Collection', 'Statuary', 'String', 'Sum', 'Tenantry', 'Third World', 'Traffic', 'Treasure', 'Treasure Trove', 'Trinketry', 'Troponymy', 'Vegetation', 'Victoriana', 'Wardrobe'], 'central': ['Bakery', 'Beehive', 'Brokerage House', 'Colliery', 'Creamery', 'Drill Site', 'Exchange', 'Farm', 'Fish Farm', 'Fishery', 'Forge', 'Gasworks', 'Glassworks', 'Ironworks', 'Job', 'Lab', 'Laundry', 'Location', 'Lumberyard', 'Oyster Bed', 'Proving Ground', 'Ropewalk', 'Roundhouse', 'Shipyard', 'Shop Floor', 'Studio', 'Tannery', 'Test Bed', 'Waterworks', 'Workshop'], 'example': ['Abstractionism', 'Antitype', 'Appearance', 'Blur', 'Concretism', 'Image', 'Instantiation', 'Interpretation', 'Memory', 'Model', 'Overlap', 'Percept', 'Phantasmagoria', 'Psychosexuality', 'Schema', 'Stereotype'], 'areas': ['Context', 'Ecology', 'Home', 'Milieu', 'Setting', 'Sphere', 'Street'], 'legal documents': ['Ballot', 'Brevet', 'Capitulation', 'Certificate', 'Charter', 'Commercial Document', 'Confession', 'Copyright', 'Enclosure', 'Form', 'Legal Document', 'Papyrus', 'Patent', 'Platform', 'Report', 'Resignation', 'Resolution', 'Source', 'Specification', 'Voucher'], 'ability': ['Attitude', 'Cognitive Factor', 'Content', 'Episteme', 'Equivalent', 'History', 'Inability', 'Information', 'Lexis', 'Mind', 'Perception', 'Place', 'Practice', 'Process', 'Public Knowledge', 'Structure', 'Vocabulary'], 'apple': ['Crab Apple', 'Wild Apple'], 'quality control': ['Inventory Control', 'Management Control', 'Quality Control'], 'marketing': ['Carriage Trade', 'Commercial Enterprise', 'Distribution', 'E-commerce', 'Evasion', 'Exchange', 'Exporting', 'Importing', 'Initial Public Offering', 'Payment', 'Selling', 'Trade', 'Trading', 'Traffic']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(mappedSents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns8fZ3WDhi23",
        "outputId": "9c5bdc42-9f0a-462d-a9e5-a0dba3a42fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'massive data sets': [\"AI's ability to process massive data sets gives enterprises insights into their operations they might not otherwise have noticed.\"], 'humans': ['AI technologies, particularly deep learning models such as artificial neural networks, can process large amounts of data much faster and make predictions more accurately than humans can.', 'It has been effectively used in business to automate tasks traditionally done by humans, including customer service, lead generation, fraud detection and quality control.', 'In a number of areas, AI can perform tasks more efficiently and accurately than humans.'], 'ai techniques': [\"As AI techniques are incorporated into more products and services, organizations must also be attuned to AI's potential to create biased and discriminatory systems, intentionally or inadvertently.\", 'Advances in AI techniques have not only helped fuel an explosion in efficiency, but also opened the door to entirely new business opportunities for some larger enterprises.'], 'alphabet': [\"AI has become central to many of today's largest and most successful companies, including Alphabet, Apple, Microsoft and Meta, which use AI to improve their operations and outpace competitors.\", 'At Alphabet subsidiary Google, for example, AI is central to its eponymous search engine, and self-driving car company Waymo began as an Alphabet division.', 'At Alphabet subsidiary Google, for example, AI is central to its eponymous search engine, and self-driving car company Waymo began as an Alphabet division.'], 'enterprises insights': [\"AI's ability to process massive data sets gives enterprises insights into their operations they might not otherwise have noticed.\"], 'number': ['In a number of areas, AI can perform tasks more efficiently and accurately than humans.'], 'relevant fields': ['It is especially useful for repetitive, detail-oriented tasks such as analyzing large numbers of legal documents to ensure relevant fields are properly filled in.'], 'company': ['Prior to the current wave of AI, for example, it would have been hard to imagine using computer software to connect riders to taxis on demand, yet Uber has become a Fortune 500 company by doing just that.', 'At Alphabet subsidiary Google, for example, AI is central to its eponymous search engine, and self-driving car company Waymo began as an Alphabet division.'], 'operations': [\"AI has become central to many of today's largest and most successful companies, including Alphabet, Apple, Microsoft and Meta, which use AI to improve their operations and outpace competitors.\", \"AI's ability to process massive data sets gives enterprises insights into their operations they might not otherwise have noticed.\"], 'tasks': ['It has been effectively used in business to automate tasks traditionally done by humans, including customer service, lead generation, fraud detection and quality control.', 'It is especially useful for repetitive, detail-oriented tasks such as analyzing large numbers of legal documents to ensure relevant fields are properly filled in.', 'In a number of areas, AI can perform tasks more efficiently and accurately than humans.'], 'customer service': ['It has been effectively used in business to automate tasks traditionally done by humans, including customer service, lead generation, fraud detection and quality control.'], 'business': ['Advances in AI techniques have not only helped fuel an explosion in efficiency, but also opened the door to entirely new business opportunities for some larger enterprises.', 'It has been effectively used in business to automate tasks traditionally done by humans, including customer service, lead generation, fraud detection and quality control.'], 'data': ['While the huge volume of data created on a daily basis would bury a human researcher, AI applications using machine learning can take that data and quickly turn it into actionable information.', 'While the huge volume of data created on a daily basis would bury a human researcher, AI applications using machine learning can take that data and quickly turn it into actionable information.', 'AI technologies, particularly deep learning models such as artificial neural networks, can process large amounts of data much faster and make predictions more accurately than humans can.', 'A primary disadvantage of AI is that it is expensive to process the large amounts of data AI requires.'], 'important': ['The rapidly expanding array of generative AI tools is also becoming important in fields ranging from education to marketing to product design.', 'AI is important for its potential to change how we live, work and play.'], 'central': [\"AI has become central to many of today's largest and most successful companies, including Alphabet, Apple, Microsoft and Meta, which use AI to improve their operations and outpace competitors.\", 'At Alphabet subsidiary Google, for example, AI is central to its eponymous search engine, and self-driving car company Waymo began as an Alphabet division.'], 'example': ['Prior to the current wave of AI, for example, it would have been hard to imagine using computer software to connect riders to taxis on demand, yet Uber has become a Fortune 500 company by doing just that.', 'At Alphabet subsidiary Google, for example, AI is central to its eponymous search engine, and self-driving car company Waymo began as an Alphabet division.'], 'lead generation': ['It has been effectively used in business to automate tasks traditionally done by humans, including customer service, lead generation, fraud detection and quality control.'], 'fraud detection': ['It has been effectively used in business to automate tasks traditionally done by humans, including customer service, lead generation, fraud detection and quality control.'], 'large amounts': ['AI technologies, particularly deep learning models such as artificial neural networks, can process large amounts of data much faster and make predictions more accurately than humans can.', 'A primary disadvantage of AI is that it is expensive to process the large amounts of data AI requires.'], 'areas': ['In a number of areas, AI can perform tasks more efficiently and accurately than humans.'], 'legal documents': ['It is especially useful for repetitive, detail-oriented tasks such as analyzing large numbers of legal documents to ensure relevant fields are properly filled in.'], 'ability': [\"AI's ability to process massive data sets gives enterprises insights into their operations they might not otherwise have noticed.\"], 'apple': [\"AI has become central to many of today's largest and most successful companies, including Alphabet, Apple, Microsoft and Meta, which use AI to improve their operations and outpace competitors.\"], 'quality control': ['It has been effectively used in business to automate tasks traditionally done by humans, including customer service, lead generation, fraud detection and quality control.'], 'marketing': ['The rapidly expanding array of generative AI tools is also becoming important in fields ranging from education to marketing to product design.']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"**************************************        Multiple Choice Questions        *******************************\")\n",
        "print()\n",
        "import re\n",
        "import random\n",
        "iterator = 1 #To keep the count of the questions\n",
        "for each in mappedDists:\n",
        "    sent=mappedSents[each][0]\n",
        "    p=re.compile(each,re.IGNORECASE) #Converts into regular expression for pattern matching\n",
        "    op=p.sub(\"________\",sent) #Replaces the keyword with underscores(blanks)\n",
        "    print(\"Question %s-> \"%(iterator),op) #Prints the question along with a question number\n",
        "    options=[each.capitalize()]+mappedDists[each] #Capitalizes the options\n",
        "    options=options[:4] #Selects only 4 options\n",
        "    opts=['a','b','c','d']\n",
        "    random.shuffle(options) #Shuffle the options so that order is not always same\n",
        "    for i,ch in enumerate(options):\n",
        "        print(\"\\t\",opts[i],\") \", ch) #Print the options\n",
        "    print()\n",
        "    iterator+=1 #Increase the counter"
      ],
      "metadata": {
        "id": "x-9LG_K-GxaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3bb65b7-a57d-48a1-9b08-e8f029deb1e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************************************        Multiple Choice Questions        *******************************\n",
            "\n",
            "Question 1->  AI technologies, particularly deep learning models such as artificial neural networks, can process large amounts of data much faster and make predictions more accurately than ________ can.\n",
            "\t a )  Homo\n",
            "\t b )  Humans\n",
            "\t c )  Dryopithecine\n",
            "\t d )  Australopithecine\n",
            "\n",
            "Question 2->  AI has become central to many of today's largest and most successful companies, including ________, Apple, Microsoft and Meta, which use AI to improve their operations and outpace competitors.\n",
            "\t a )  Alphabet\n",
            "\t b )  Alphanumerics\n",
            "\n",
            "Question 3->  In a ________ of areas, AI can perform tasks more efficiently and accurately than humans.\n",
            "\t a )  Number\n",
            "\t b )  All Clear\n",
            "\t c )  Alarm\n",
            "\t d )  Animal Communication\n",
            "\n",
            "Question 4->  Prior to the current wave of AI, for example, it would have been hard to imagine using computer software to connect riders to taxis on demand, yet Uber has become a Fortune 500 ________ by doing just that.\n",
            "\t a )  Caller\n",
            "\t b )  Guest\n",
            "\t c )  Company\n",
            "\t d )  Boulevardier\n",
            "\n",
            "Question 5->  AI has become central to many of today's largest and most successful companies, including Alphabet, Apple, Microsoft and Meta, which use AI to improve their ________ and outpace competitors.\n",
            "\t a )  Attempt\n",
            "\t b )  Acting\n",
            "\t c )  Operations\n",
            "\t d )  Aid\n",
            "\n",
            "Question 6->  It has been effectively used in business to automate ________ traditionally done by humans, including customer service, lead generation, fraud detection and quality control.\n",
            "\t a )  Busywork\n",
            "\t b )  Action\n",
            "\t c )  Care\n",
            "\t d )  Tasks\n",
            "\n",
            "Question 7->  Advances in AI techniques have not only helped fuel an explosion in efficiency, but also opened the door to entirely new ________ opportunities for some larger enterprises.\n",
            "\t a )  Collective\n",
            "\t b )  Giant\n",
            "\t c )  Commercial Enterprise\n",
            "\t d )  Business\n",
            "\n",
            "Question 8->  While the huge volume of ________ created on a daily basis would bury a human researcher, AI applications using machine learning can take that ________ and quickly turn it into actionable information.\n",
            "\t a )  Data\n",
            "\t b )  Agglomeration\n",
            "\t c )  Armamentarium\n",
            "\t d )  Ana\n",
            "\n",
            "Question 9->  AI has become ________ to many of today's largest and most successful companies, including Alphabet, Apple, Microsoft and Meta, which use AI to improve their operations and outpace competitors.\n",
            "\t a )  Beehive\n",
            "\t b )  Bakery\n",
            "\t c )  Central\n",
            "\t d )  Brokerage House\n",
            "\n",
            "Question 10->  Prior to the current wave of AI, for ________, it would have been hard to imagine using computer software to connect riders to taxis on demand, yet Uber has become a Fortune 500 company by doing just that.\n",
            "\t a )  Antitype\n",
            "\t b )  Example\n",
            "\t c )  Appearance\n",
            "\t d )  Abstractionism\n",
            "\n",
            "Question 11->  In a number of ________, AI can perform tasks more efficiently and accurately than humans.\n",
            "\t a )  Ecology\n",
            "\t b )  Context\n",
            "\t c )  Areas\n",
            "\t d )  Home\n",
            "\n",
            "Question 12->  It is especially useful for repetitive, detail-oriented tasks such as analyzing large numbers of ________ to ensure relevant fields are properly filled in.\n",
            "\t a )  Ballot\n",
            "\t b )  Brevet\n",
            "\t c )  Legal documents\n",
            "\t d )  Capitulation\n",
            "\n",
            "Question 13->  AI's ________ to process massive data sets gives enterprises insights into their operations they might not otherwise have noticed.\n",
            "\t a )  Ability\n",
            "\t b )  Content\n",
            "\t c )  Cognitive Factor\n",
            "\t d )  Attitude\n",
            "\n",
            "Question 14->  AI has become central to many of today's largest and most successful companies, including Alphabet, ________, Microsoft and Meta, which use AI to improve their operations and outpace competitors.\n",
            "\t a )  Apple\n",
            "\t b )  Crab Apple\n",
            "\t c )  Wild Apple\n",
            "\n",
            "Question 15->  It has been effectively used in business to automate tasks traditionally done by humans, including customer service, lead generation, fraud detection and ________.\n",
            "\t a )  Inventory Control\n",
            "\t b )  Management Control\n",
            "\t c )  Quality Control\n",
            "\t d )  Quality control\n",
            "\n",
            "Question 16->  The rapidly expanding array of generative AI tools is also becoming important in fields ranging from education to ________ to product design.\n",
            "\t a )  Marketing\n",
            "\t b )  Commercial Enterprise\n",
            "\t c )  Distribution\n",
            "\t d )  Carriage Trade\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "id": "GgZds7MdU_Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2"
      ],
      "metadata": {
        "id": "BpNnQIvJczr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdfFileObj = open('/content/AM_NLP_Unit3.pdf', 'rb')"
      ],
      "metadata": {
        "id": "R_oDhrjfU_7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdfFileObj"
      ],
      "metadata": {
        "id": "hyE9eBdcppdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pageObj = pdfReader.pages[27]"
      ],
      "metadata": {
        "id": "MvxjuONYdQPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdfReader = PyPDF2.PdfReader(pdfFileObj)"
      ],
      "metadata": {
        "id": "NtKYUDcrU_94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pdfReader)"
      ],
      "metadata": {
        "id": "bnsGrFLaigYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(pdfReader.pages))"
      ],
      "metadata": {
        "id": "7LMpvNyrVAAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pageObj.extract_text())"
      ],
      "metadata": {
        "id": "6NNFLjzPU_J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdfFileObj.close()"
      ],
      "metadata": {
        "id": "XMAlE-oyU_Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1=pageObj.extract_text()"
      ],
      "metadata": {
        "id": "yGSD2FMOU_Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text1)"
      ],
      "metadata": {
        "id": "O3tINNRuf7hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ORONQKuhf7u8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}